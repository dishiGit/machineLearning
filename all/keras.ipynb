{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "np.set_printoptions(threshold=20000)\n",
    "\n",
    "# load data\n",
    "image_data='train_images.npy'\n",
    "label_data='train_labels.csv'\n",
    "\n",
    "# images = np.load(image_data, encoding='latin1')\n",
    "images = np.load(image_data, encoding='latin1')[:,1]\n",
    "labels = np.loadtxt(label_data, dtype=str, encoding='latin1', delimiter=',')[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[]\n",
    "# for i in range(len(images)):\n",
    "#     image1 = (images[i][1]).reshape(100,100)\n",
    "#     #plt.imshow(image1)\n",
    "#     img = cv2.imwrite('temp.jpg',image1)\n",
    "#     img = cv2.imread('temp.jpg',0)\n",
    "#     edges = cv2.Canny(img, 0, 100)\n",
    "#     #plt.imshow(edges)\n",
    "\n",
    "#     im2, contours, hierarchy = cv2.findContours(edges, cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "\n",
    "\n",
    "#     mask=np.zeros(img.shape, np.uint8)\n",
    "#     cv2.drawContours(mask, cnts, -1, (255),1)\n",
    "#     a.append(mask)\n",
    "#     #plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to \"2d\" form\n",
    "# print(images[0])\n",
    "print(images.shape)\n",
    "X = np.reshape(images.tolist(), (-1,100,100,1))\n",
    "X = X.astype('float32')/255\n",
    "\n",
    "\n",
    "# one-hot encoding for y\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y_encoded = encoder.transform(labels)\n",
    "y = keras.utils.to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 128)     32896     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 128)     4194432   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       409728    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 25, 256)       131328    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 25, 25, 256)       262400    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               40960256  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 31)                7967      \n",
      "=================================================================\n",
      "Total params: 46,703,903\n",
      "Trainable params: 46,703,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, 16, input_shape=(100,100,1), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, 16, padding='same', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(128, 7, padding='same', activation='relu'))\n",
    "# model.add(Conv2D(128, 7, padding='same', activation='relu'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(128, 5, padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "# model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, 3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, 2, padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, 2, padding='same', activation='relu'))\n",
    "# model.add(Conv2D(256, 2, padding='same', activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 80s 10ms/step - loss: 3.4151 - acc: 0.0510 - val_loss: 3.3972 - val_acc: 0.0575\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3914 - acc: 0.0528 - val_loss: 3.3902 - val_acc: 0.0575\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3866 - acc: 0.0541 - val_loss: 3.3864 - val_acc: 0.0575\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3875 - acc: 0.0543 - val_loss: 3.3830 - val_acc: 0.0575\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3833 - acc: 0.0570 - val_loss: 3.3822 - val_acc: 0.0575\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3827 - acc: 0.0574 - val_loss: 3.3851 - val_acc: 0.0575\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3819 - acc: 0.0551 - val_loss: 3.3842 - val_acc: 0.0575\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3801 - acc: 0.0580 - val_loss: 3.3841 - val_acc: 0.0575\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3816 - acc: 0.0573 - val_loss: 3.3828 - val_acc: 0.0575\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3802 - acc: 0.0569 - val_loss: 3.3829 - val_acc: 0.0575\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3804 - acc: 0.0566 - val_loss: 3.3835 - val_acc: 0.0575\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3786 - acc: 0.0570 - val_loss: 3.3837 - val_acc: 0.0575\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3803 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3793 - acc: 0.0570 - val_loss: 3.3831 - val_acc: 0.0575\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3785 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3789 - acc: 0.0570 - val_loss: 3.3828 - val_acc: 0.0575\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3779 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3789 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3779 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3784 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3776 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 3.3775 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 3.3773 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3781 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3772 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3778 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3779 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3780 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3771 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3775 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3770 - acc: 0.0570 - val_loss: 3.3828 - val_acc: 0.0575\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3776 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3771 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3779 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3775 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3778 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3771 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3773 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3773 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3770 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3768 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3769 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3762 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3766 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3767 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3765 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3823 - val_acc: 0.0575\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3763 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3827 - val_acc: 0.0575\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 3.3764 - acc: 0.0570 - val_loss: 3.3826 - val_acc: 0.0575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18beefcac88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_split=0.2, epochs=150, batch_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 337.73%\n",
      "Test Accuracy: 5.71%\n"
     ]
    }
   ],
   "source": [
    "scores1 = model.evaluate(X, y, verbose=0)\n",
    "print(\"Test Loss: %.2f%%\" % (scores1[0]*100))\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-73e34c6dde10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maxarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'AxesSubplot' object does not support indexing"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 4)\n",
    "f.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(4):\n",
    "    axarr[0][i].imshow(np.reshape(X[i],(100,100)), cmap='hot', interpolation='nearest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
