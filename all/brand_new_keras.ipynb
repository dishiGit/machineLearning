{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten, AlphaDropout, SpatialDropout2D, Cropping2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "np.set_printoptions(threshold=20000)\n",
    "\n",
    "# load data\n",
    "image_data='train_images.npy'\n",
    "label_data='train_labels.csv'\n",
    "\n",
    "# images = np.load(image_data, encoding='latin1')\n",
    "images = np.load(image_data, encoding='latin1')[:,1]\n",
    "labels = np.loadtxt(label_data, dtype=str, encoding='latin1', delimiter=',')[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[]\n",
    "# for i in range(len(images)):\n",
    "#     image1 = (images[i][1]).reshape(100,100)\n",
    "#     #plt.imshow(image1)\n",
    "#     img = cv2.imwrite('temp.jpg',image1)\n",
    "#     img = cv2.imread('temp.jpg',0)\n",
    "#     edges = cv2.Canny(img, 0, 100)\n",
    "#     #plt.imshow(edges)\n",
    "\n",
    "#     im2, contours, hierarchy = cv2.findContours(edges, cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "\n",
    "\n",
    "#     mask=np.zeros(img.shape, np.uint8)\n",
    "#     cv2.drawContours(mask, cnts, -1, (255),1)\n",
    "#     a.append(mask)\n",
    "#     #plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.        ],\n",
       "          [0.39215687],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.00392157],\n",
       "          [0.50980395],\n",
       "          [1.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]],\n",
       " \n",
       " \n",
       "        [[[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]],\n",
       " \n",
       "         [[0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          ...,\n",
       "          [0.        ],\n",
       "          [0.        ],\n",
       "          [0.        ]]]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape X to \"2d\" form\n",
    "# print(images[0])\n",
    "# print(images.shape)\n",
    "X = np.reshape(images.tolist(), (-1,100,100,1))\n",
    "X = X.astype('float32')/255\n",
    "\n",
    "\n",
    "# one-hot encoding for y\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "y_encoded = encoder.transform(labels)\n",
    "y = keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "shuffle(X,y)\n",
    "# X = X[:1000]\n",
    "# y = y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_1 (Cropping2D)    (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 16)        800       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 16)        6416      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 32)          4128      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31)                35743     \n",
      "=================================================================\n",
      "Total params: 67,391\n",
      "Trainable params: 67,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Cropping2D(2, input_shape=(100,100,1)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Conv2D(16, 7, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Conv2D(16, 5, padding='same', activation='relu'))\n",
    "model.add(Conv2D(16, 5, padding='same', activation='relu'))\n",
    "model.add(Conv2D(16, 5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "# model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, 2, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Conv2D(32, 2, padding='same', activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(200, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "epochs = 20\n",
    "lrate = 0.0085\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=1e-8, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 4s 554us/step - loss: 3.3925 - acc: 0.0525 - val_loss: 3.3909 - val_acc: 0.0400\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 3.3820 - acc: 0.0545 - val_loss: 3.3857 - val_acc: 0.0535\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 3.3811 - acc: 0.0561 - val_loss: 3.3824 - val_acc: 0.0575\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 3.3805 - acc: 0.0553 - val_loss: 3.3840 - val_acc: 0.0575\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 3.3796 - acc: 0.0563 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 3.3791 - acc: 0.0569 - val_loss: 3.3844 - val_acc: 0.0575\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 3.3792 - acc: 0.0570 - val_loss: 3.3828 - val_acc: 0.0575\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 3.3793 - acc: 0.0570 - val_loss: 3.3832 - val_acc: 0.0575\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 3.3786 - acc: 0.0570 - val_loss: 3.3820 - val_acc: 0.0575\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 3.3783 - acc: 0.0570 - val_loss: 3.3825 - val_acc: 0.0575\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 3.3782 - acc: 0.0570 - val_loss: 3.3853 - val_acc: 0.0575\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 3.3740 - acc: 0.0566 - val_loss: 3.3696 - val_acc: 0.0585\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 3.3264 - acc: 0.0684 - val_loss: 3.2850 - val_acc: 0.0905\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 3.1959 - acc: 0.0971 - val_loss: 3.1188 - val_acc: 0.1060\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 3.0918 - acc: 0.1022 - val_loss: 3.1297 - val_acc: 0.0965\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 3.0436 - acc: 0.1000 - val_loss: 3.1046 - val_acc: 0.0995\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 3.0037 - acc: 0.1146 - val_loss: 3.0482 - val_acc: 0.0985\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 2.9750 - acc: 0.1215 - val_loss: 3.0265 - val_acc: 0.1100\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 2.9487 - acc: 0.1335 - val_loss: 3.0796 - val_acc: 0.0955\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 2.9266 - acc: 0.1396 - val_loss: 2.9992 - val_acc: 0.1100\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 2.8752 - acc: 0.1539 - val_loss: 2.9088 - val_acc: 0.1500\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 2.7744 - acc: 0.1875 - val_loss: 2.8897 - val_acc: 0.1455\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 2.6559 - acc: 0.2191 - val_loss: 2.6878 - val_acc: 0.2010\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 2.5139 - acc: 0.2469 - val_loss: 2.7687 - val_acc: 0.1775\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 2.3774 - acc: 0.2814 - val_loss: 2.4007 - val_acc: 0.2610\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 2.2846 - acc: 0.3114 - val_loss: 2.4106 - val_acc: 0.2750\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 2.1998 - acc: 0.3431 - val_loss: 2.2043 - val_acc: 0.3170\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 2.1164 - acc: 0.3654 - val_loss: 2.1055 - val_acc: 0.3540\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 2.0508 - acc: 0.3818 - val_loss: 2.1715 - val_acc: 0.3315\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.9853 - acc: 0.4071 - val_loss: 2.0616 - val_acc: 0.3880\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.9032 - acc: 0.4271 - val_loss: 1.9463 - val_acc: 0.4230\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.8425 - acc: 0.4454 - val_loss: 1.9681 - val_acc: 0.4090\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.7799 - acc: 0.4645 - val_loss: 1.8722 - val_acc: 0.4435\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 1.7084 - acc: 0.4862 - val_loss: 1.8103 - val_acc: 0.4640\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.6784 - acc: 0.4940 - val_loss: 1.7717 - val_acc: 0.4830\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.6201 - acc: 0.5124 - val_loss: 1.6410 - val_acc: 0.5290\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 1.5948 - acc: 0.5227 - val_loss: 1.7208 - val_acc: 0.4855\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.5369 - acc: 0.5330 - val_loss: 1.6995 - val_acc: 0.4990\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.4956 - acc: 0.5521 - val_loss: 1.7217 - val_acc: 0.4905\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 1.4658 - acc: 0.5536 - val_loss: 1.7014 - val_acc: 0.5165\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.4461 - acc: 0.5614 - val_loss: 1.5773 - val_acc: 0.5375\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.4251 - acc: 0.5754 - val_loss: 1.6303 - val_acc: 0.5245\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 1.3727 - acc: 0.5859 - val_loss: 1.5970 - val_acc: 0.5280\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.3462 - acc: 0.5887 - val_loss: 1.6073 - val_acc: 0.5330\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.3259 - acc: 0.5980 - val_loss: 1.5714 - val_acc: 0.5395\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.3138 - acc: 0.5959 - val_loss: 1.4909 - val_acc: 0.5675\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.2850 - acc: 0.6050 - val_loss: 1.5546 - val_acc: 0.5475\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 1.2736 - acc: 0.6146 - val_loss: 1.5145 - val_acc: 0.5495\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 3s 320us/step - loss: 1.2489 - acc: 0.6179 - val_loss: 1.4857 - val_acc: 0.5735\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 3s 320us/step - loss: 1.2265 - acc: 0.6234 - val_loss: 1.5034 - val_acc: 0.5615\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.2038 - acc: 0.6310 - val_loss: 1.4937 - val_acc: 0.5555\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 1.2144 - acc: 0.6219 - val_loss: 1.4765 - val_acc: 0.5570\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.1791 - acc: 0.6332 - val_loss: 1.4423 - val_acc: 0.5745\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 1.1606 - acc: 0.6412 - val_loss: 1.5035 - val_acc: 0.5625\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.1351 - acc: 0.6388 - val_loss: 1.4947 - val_acc: 0.5630\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 1.1389 - acc: 0.6426 - val_loss: 1.4928 - val_acc: 0.5845\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.1287 - acc: 0.6466 - val_loss: 1.4819 - val_acc: 0.5660\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 1.1324 - acc: 0.6464 - val_loss: 1.4971 - val_acc: 0.5645\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 3s 338us/step - loss: 1.0901 - acc: 0.6605 - val_loss: 1.4255 - val_acc: 0.5850\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 322us/step - loss: 1.0914 - acc: 0.6602 - val_loss: 1.4837 - val_acc: 0.5560\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.0805 - acc: 0.6589 - val_loss: 1.4451 - val_acc: 0.5695\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.0569 - acc: 0.6655 - val_loss: 1.3851 - val_acc: 0.6030\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.0683 - acc: 0.6610 - val_loss: 1.4218 - val_acc: 0.5905\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 3s 320us/step - loss: 1.0691 - acc: 0.6664 - val_loss: 1.4739 - val_acc: 0.5475\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 1.0386 - acc: 0.6705 - val_loss: 1.4903 - val_acc: 0.5730\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 1.0227 - acc: 0.6720 - val_loss: 1.4794 - val_acc: 0.5735\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.0143 - acc: 0.6768 - val_loss: 1.4435 - val_acc: 0.5910\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.0020 - acc: 0.6823 - val_loss: 1.5360 - val_acc: 0.5760\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.0026 - acc: 0.6839 - val_loss: 1.4430 - val_acc: 0.5925\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.0032 - acc: 0.6844 - val_loss: 1.3678 - val_acc: 0.6100\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 0.9783 - acc: 0.6926 - val_loss: 1.4048 - val_acc: 0.5990\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 0.9769 - acc: 0.6891 - val_loss: 1.5076 - val_acc: 0.5750\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.9624 - acc: 0.6890 - val_loss: 1.4482 - val_acc: 0.5775\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.9542 - acc: 0.6952 - val_loss: 1.4065 - val_acc: 0.6000\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.9474 - acc: 0.6955 - val_loss: 1.4708 - val_acc: 0.5665\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.9318 - acc: 0.6989 - val_loss: 1.4705 - val_acc: 0.5800\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 0.9174 - acc: 0.7072 - val_loss: 1.4951 - val_acc: 0.5815\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 0.9314 - acc: 0.7031 - val_loss: 1.4363 - val_acc: 0.5815\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.9187 - acc: 0.7015 - val_loss: 1.4444 - val_acc: 0.5990\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 0.9082 - acc: 0.7113 - val_loss: 1.5267 - val_acc: 0.5915\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.8927 - acc: 0.7229 - val_loss: 1.4870 - val_acc: 0.5805\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.9119 - acc: 0.7106 - val_loss: 1.4303 - val_acc: 0.5960\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 0.9133 - acc: 0.7066 - val_loss: 1.5199 - val_acc: 0.5720\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.9018 - acc: 0.7066 - val_loss: 1.4592 - val_acc: 0.6010\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.8945 - acc: 0.7126 - val_loss: 1.5369 - val_acc: 0.5835\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 0.8731 - acc: 0.7172 - val_loss: 1.4777 - val_acc: 0.6070\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 3s 315us/step - loss: 0.8937 - acc: 0.7109 - val_loss: 1.4868 - val_acc: 0.5895\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 0.8815 - acc: 0.7162 - val_loss: 1.4844 - val_acc: 0.5950\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 0.8644 - acc: 0.7168 - val_loss: 1.4753 - val_acc: 0.5940\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 0.8672 - acc: 0.7194 - val_loss: 1.4884 - val_acc: 0.5830\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 0.8404 - acc: 0.7300 - val_loss: 1.4919 - val_acc: 0.5820\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 0.8818 - acc: 0.7181 - val_loss: 1.4772 - val_acc: 0.5995\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 0.8629 - acc: 0.7249 - val_loss: 1.4611 - val_acc: 0.6030\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 0.8319 - acc: 0.7249 - val_loss: 1.5257 - val_acc: 0.6010\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 0.8297 - acc: 0.7329 - val_loss: 1.4446 - val_acc: 0.6045\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 0.8596 - acc: 0.7225 - val_loss: 1.4663 - val_acc: 0.6170\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.8501 - acc: 0.7294 - val_loss: 1.5001 - val_acc: 0.6020\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 0.8193 - acc: 0.7391 - val_loss: 1.5472 - val_acc: 0.5700\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.8352 - acc: 0.7291 - val_loss: 1.4421 - val_acc: 0.5910\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 0.8276 - acc: 0.7322 - val_loss: 1.4773 - val_acc: 0.5985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2fed28780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_split=0.2, epochs=100, batch_size=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 71.62%\n",
      "Test Accuracy: 80.71%\n"
     ]
    }
   ],
   "source": [
    "scores1 = model.evaluate(X, y, verbose=0)\n",
    "print(\"Test Loss: %.2f%%\" % (scores1[0]*100))\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-73e34c6dde10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maxarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'AxesSubplot' object does not support indexing"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 4)\n",
    "f.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(4):\n",
    "    axarr[0][i].imshow(np.reshape(X[i],(100,100)), cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Id' 'Category']\n",
      " ['0' 'pineapple']\n",
      " ['1' 'nose']\n",
      " ...\n",
      " ['9997' 'skateboard']\n",
      " ['9998' 'pineapple']\n",
      " ['9999' 'mug']]\n"
     ]
    }
   ],
   "source": [
    "# produce prediction file\n",
    "test_array = np.load('test_images.npy',encoding='latin1')[:,1]\n",
    "X_test = np.reshape(test_array.tolist(), (-1,100,100,1))\n",
    "X_test = X_test.astype('float32')/255\n",
    "result = np.loadtxt('sample_submission.csv',dtype=str, encoding='latin1',delimiter=',')\n",
    "y = model.predict(X_test)\n",
    "sparse_y = [np.argmax(pred) for pred in y]\n",
    "decoded_y = encoder.inverse_transform(sparse_y)\n",
    "for i in range(len(decoded_y)):\n",
    "    result[i+1][1]=decoded_y[i]\n",
    "\n",
    "print(result)\n",
    "    \n",
    "np.savetxt('k_sgd_submission.csv', result, delimiter=',', fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"keras_1_X.npy\", X)\n",
    "np.save(\"keras_1_y.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce prediction file\n",
    "test_array = np.load('train_images.npy',encoding='latin1')[:,1]\n",
    "X_test = np.reshape(test_array.tolist(), (-1,100,100,1))\n",
    "X_test = X_test.astype('float32')/255\n",
    "result = np.loadtxt('sample_submission.csv',dtype=str, encoding='latin1',delimiter=',')\n",
    "y = model.predict(X_test)\n",
    "# print(y)\n",
    "sparse_y = [np.argmax(pred) for pred in y]\n",
    "# print(sparse_y)\n",
    "decoded_y = encoder.inverse_transform(sparse_y)\n",
    "# print(decoded_y)\n",
    "# print(result)\n",
    "for i in range(len(decoded_y)):\n",
    "    result[i+1][1]=decoded_y[i]\n",
    "\n",
    "# print(result)\n",
    "    \n",
    "np.savetxt('train_pred.csv', result, delimiter=',', fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
