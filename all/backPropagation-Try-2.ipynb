{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "#import opencv as cv2\n",
    "\n",
    "img_array = np.load('test_images.npy',encoding='latin1')\n",
    "\n",
    "images = np.load('train_images.npy',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(path):\n",
    "    with open(path) as csv_file:\n",
    "        result = []\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            line = []\n",
    "            for data in row:\n",
    "                line.append(data)\n",
    "            result.append(line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input=[]\n",
    "for i in range(len(images)):\n",
    "    image1 = (images[i][1]).reshape(100,100)\n",
    "    #plt.imshow(image1)\n",
    "    img = cv2.imwrite('temp.jpg',image1)\n",
    "    img = cv2.imread('temp.jpg',0)\n",
    "    edges = cv2.Canny(img, 0, 100)\n",
    "    #plt.imshow(edges)\n",
    "\n",
    "    im2, contours, hierarchy = cv2.findContours(edges, cv2.RETR_CCOMP,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "\n",
    "\n",
    "    mask=np.zeros(img.shape)\n",
    "    cv2.drawContours(mask, cnts, -1, (255),1)\n",
    "    input.append(mask)\n",
    "    #plt.imshow(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judyy\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "labels = np.array(readCSV('train_labels.csv'))#np.loadtxt('train_labels.csv',dtype=str,delimiter=',')\n",
    "X = [x.reshape(10000,) for x in input]\n",
    "y = labels[1:,1].tolist()\n",
    "\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y_encoded = encoder.transform(y)\n",
    "print(set(y_encoded))\n",
    "# y = np.loadtxt(label_data, dtype=str, encoding='latin1', delimiter=',')[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage.measure\n",
    "x = [skimage.measure.block_reduce(a, (4,4), np.max) for a in input]\n",
    "print(len(x[0]))\n",
    "# X = [a.reshape(625,1) for a in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [ 255.],\n",
      "       [ 255.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.],\n",
      "       [   0.]]), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train = []\n",
    "# for i in range(len(X)):\n",
    "#     a = []\n",
    "#     for k in X[i]:\n",
    "#         a.append([k/255])\n",
    "#     train.append([a,[y_encoded[i]]])\n",
    "# print(train[0])\n",
    "\n",
    "data_y = [preprocess_y(y) for y in y_encoded]\n",
    "data_x = [a.reshape((625,1)) for a in x]\n",
    "full_data = list(zip(data_x, data_y))\n",
    "random.shuffle(full_data)\n",
    "print(full_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do an 80/20 split for training/test: \n",
    "num_examples = len(full_data)\n",
    "split = int(0.8 * num_examples)\n",
    "train_data = full_data[:split]\n",
    "test_data = full_data[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def preprocess_y(y):\n",
    "    y = int(y)\n",
    "    new_y = np.zeros(31)\n",
    "    new_y[y] = 1.0\n",
    "    return new_y.T.reshape((31,1))\n",
    "\n",
    "\n",
    "def sigmoid(z, derivative):\n",
    "    sig = 1.0 / (1.0 + np.exp(-z))\n",
    "    if derivative == False:\n",
    "        return sig\n",
    "    else:\n",
    "        return sig * (1 - sig)   \n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes, norm, mom, lr):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.__init__weights()\n",
    "        self.__init__biases()\n",
    "        self.momentum_coef = mom\n",
    "        self.norm = norm\n",
    "        self.lr = lr\n",
    "        \n",
    "    def __init__weights(self):\n",
    "        #weights are initialized using Glorot initialization\n",
    "        self.weights = [np.random.uniform(-np.sqrt(6)/np.sqrt(x+y), np.sqrt(6)/np.sqrt(x+y),(y, x)) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def __init__biases(self):\n",
    "        #Biases are initialized randomly\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "  \n",
    "    #Feed Forward method computes the output vector of Neural Network assuming that the weights are trained\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = np.dot(w, a) + b\n",
    "            a = sigmoid(a,derivative = False)\n",
    "        return a\n",
    "    \n",
    "    def Stochasitc_Gradient_Descent(self, training_data, epochs, mini_batch_size, test_data = None):\n",
    "        learning_rate = self.lr\n",
    "        best_result = 0\n",
    "        n = len(training_data)\n",
    "        if test_data != None: \n",
    "            n_test = len(test_data)\n",
    "        test_results = [0.0]    \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[k : k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            #Momentum Terms used to speed up crossing saddle points\n",
    "            gradient_biases_old = [np.zeros(b.shape) for b in self.biases]\n",
    "            gradient_weights_old = [np.zeros(w.shape) for w in self.weights]\n",
    "            \n",
    "            for batch in batches:\n",
    "                gradient_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "                gradient_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "                \n",
    "                #Momentum addition of previous gradient values\n",
    "                momentum_biases = np.multiply(self.momentum_coef,gradient_biases_old)\n",
    "                momentum_weights = np.multiply(self.momentum_coef,gradient_weights_old)\n",
    "                \n",
    "                #compute dC/dW for each example in batch and add them to gradent arrays\n",
    "                for x, y in batch:\n",
    "                    d_gradient_biases, d_gradient_weights = self.Compute_Gradient_Term(x, y)\n",
    "                    gradent_biases = [gb + dgb for gb, dgb in zip(gradient_biases, d_gradient_biases)]\n",
    "                    gradient_weights = [gw + dgw for gw, dgw in zip(gradient_weights, d_gradient_weights)]\n",
    "\n",
    "                #Standard terms of the gradient update\n",
    "                prefactor = float(-learning_rate / len(batch))\n",
    "                gradient_biases  = np.multiply(prefactor, gradient_biases)\n",
    "                gradient_weights  = np.multiply(prefactor, gradient_weights)\n",
    "                \n",
    "                #Update previous gradient updates to current gradients\n",
    "                gradient_biases_old = gradient_biases\n",
    "                gradient_weights_old = gradient_weights\n",
    "                \n",
    "                #Add update to weights and continue on next mini-batch\n",
    "                self.weights = [(1-((self.norm*learning_rate)/len(batch)))*w + gw + mw  for w, gw, mw in zip(self.weights,gradient_weights, momentum_weights)]\n",
    "                self.biases = [b + gb + mb for b, gb, mb in zip(self.biases,gradient_biases, momentum_biases)]\n",
    "\n",
    "            \n",
    "            if test_data:\n",
    "                test = self.evaluate(test_data)\n",
    "                print (\"Epoch %d: %f\" %(j+1, test/float(n_test)))\n",
    "                test_results.append(test)\n",
    "                #Adaptive Learning Rate:\n",
    "                result_improvement = np.abs(test_results[-1] - test_results[-2])/ float(test_results[-2])\n",
    "                #if there is little improvement from previous to current result then halve the learning rate\n",
    "                if result_improvement < 0.001:\n",
    "                    learning_rate = learning_rate / 2\n",
    "                    #end the reduction of the learning rate\n",
    "                    if learning_rate < 0.0005:\n",
    "                        learning_rate = 0.0005\n",
    "            else:\n",
    "                print (\"Epoch %d complete\" %(j+1))\n",
    "        print (\"Training Completed!\")\n",
    "        return test_results\n",
    "\n",
    "    def Compute_Gradient_Term(self, x, y):\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #Feed Forward Step: pass the input into the neural network with the current version of the weights\n",
    "        #and biases in order to obtain both the outputs at each layer and the activations at each layer\n",
    "        activations = [x] \n",
    "        outputs = [] \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "#             print('w')\n",
    "#             print(w)\n",
    "#             print('x')\n",
    "#             print(x.shape)\n",
    "#             print('dot')\n",
    "#             print(np.dot(w,x))\n",
    "#             print('b')\n",
    "#             print(b)\n",
    "            x = np.dot(w, x)+b\n",
    "            outputs.append(x)\n",
    "#             print('output')\n",
    "#             print(x)\n",
    "            x = sigmoid(x, derivative = False)\n",
    "            activations.append(x)\n",
    "#             print('act')\n",
    "#             print(x)\n",
    "        #Backward Pass: back propogate the errors of the previous forward pass step in order to compute\n",
    "        #the gradient updates for this specific image in the mini-batch\n",
    "        #The gradients of biases and weights will be averaged across the mini-batch and the update will be applied.\n",
    "#         print(activations[-1] - y)\n",
    "#         print(activations[-1])\n",
    "        \n",
    "        delta = (activations[-1] - y) * sigmoid(outputs[-1], derivative = True)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].T)\n",
    "        for n in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-n+1].T, delta) * sigmoid(outputs[-n], derivative = True)\n",
    "            grad_b[-n] = delta\n",
    "            grad_w[-n] = np.dot(delta, np.transpose(activations[-n-1]))\n",
    "        return (grad_b, grad_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        results = sum(int(x == y) for (x, y) in test_results)\n",
    "        return results\n",
    "    \n",
    "    def save_values(self):\n",
    "        np.savetxt(\"../best_weights_0.txt\",self.weights[0])\n",
    "        np.savetxt(\"../best_weights_1.txt\",self.weights[1])\n",
    "        np.savetxt(\"../best_biases_0.txt\",self.biases[0])\n",
    "        np.savetxt(\"../best_biases_1.txt\",self.biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Network 1 on 8000 preprocessed training images\n",
      "Epoch 1: 0.027500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judyy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 0.029000\n",
      "Epoch 3: 0.031000\n",
      "Epoch 4: 0.033000\n",
      "Epoch 5: 0.036500\n",
      "Epoch 6: 0.038500\n",
      "Epoch 7: 0.039000\n",
      "Epoch 8: 0.040000\n",
      "Epoch 9: 0.041500\n",
      "Epoch 10: 0.041500\n",
      "Epoch 11: 0.041500\n",
      "Epoch 12: 0.042000\n",
      "Epoch 13: 0.041000\n",
      "Epoch 14: 0.041000\n",
      "Epoch 15: 0.041000\n",
      "Epoch 16: 0.041000\n",
      "Epoch 17: 0.040500\n",
      "Epoch 18: 0.040500\n",
      "Epoch 19: 0.040500\n",
      "Epoch 20: 0.040500\n",
      "Epoch 21: 0.040500\n",
      "Epoch 22: 0.040500\n",
      "Epoch 23: 0.040500\n",
      "Epoch 24: 0.040500\n",
      "Epoch 25: 0.040500\n",
      "Epoch 26: 0.040500\n",
      "Epoch 27: 0.040500\n",
      "Epoch 28: 0.040500\n",
      "Epoch 29: 0.040500\n",
      "Epoch 30: 0.040500\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "print (\"Training Network 1 on %d preprocessed training images\" % len(train_data))\n",
    "shape_1 = [625, 300, 31]\n",
    "net_1 = Network(shape_1, 0.1, 0.1, 0.1)\n",
    "ModMNIST_results = net_1.Stochasitc_Gradient_Descent(train_data, 30, 1000, test_data) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
